{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"# Use networkx library to represent graphs efficiently & numpy for matrix operations & time for system time\nimport networkx as nx\nimport numpy as np\nimport time as t\nimport scipy as sp\nfrom scipy.sparse import csr_matrix\nfrom scipy.sparse import dok_matrix","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to return the PageRank of all nodes in a given webgraph G using the basic iterative method\ndef PageRankIterative(G, damping_factor = 0.85, error_tolerance = 0.1, max_iterations = 100):\n# Initialise number of iterations of algorithm to 0\n    number_iterations = 0\n# Initialise PageRank of all nodes to be initially the same value i.e 1/number_of_nodes\n    input_ranks = []\n    number_of_nodes = G.number_of_nodes()\n    for i in range(0,number_of_nodes):\n        input_ranks.append(1/number_of_nodes)\n# Iterations of PageRank Algorithm\n    while number_iterations <= max_iterations:\n        number_iterations = number_iterations + 1\n        output_ranks = []\n# Initialise PageRanks of next iteration to 0 for all nodes\n        for i in range(0,number_of_nodes):\n            output_ranks.append(0) \n# Compute PageRank iteratively node by node\n        for i in range(0,number_of_nodes):\n            number_out_edges = G.out_degree(i+1)\n# Take care of dangling nodes by distributing its PageRank evenly to all nodes(randomly move to any node)\n            if number_out_edges == 0:\n                for j in range(0,number_of_nodes):\n                    output_ranks[j] = output_ranks[j] + (input_ranks[i]/number_of_nodes)\n# Find list of nodes which have a link to the current node\n            number_in_edges = G.in_degree(i+1)\n            predecessors = list(G.predecessors(i+1))\n# Increment PageRank of current node to sum of PageRanks of all nodes linked to current node divided by number of outward links for each of those nodes\n            for j in range(0,number_in_edges):\n                output_ranks[i] = output_ranks[i] + (input_ranks[predecessors[j]-1]/G.out_degree(predecessors[j]))\n# Introduce the damping factor to increase convergence rate and ensure sum of all PageRanks is 1 at all iterations\n        for i in range(0,number_of_nodes):\n            output_ranks[i] = (damping_factor * output_ranks[i]) + ((1-damping_factor)/number_of_nodes)\n# Calculate difference between PageRanks of current and next iteration to check for convergence\n        difference = 0.0\n        for i in range(0,number_of_nodes):\n            difference = difference + abs(output_ranks[i] - input_ranks[i])\n        #print(difference)\n# Update PageRanks\n        input_ranks = output_ranks\n# Check for convergence against a tolerance value\n        if(difference < error_tolerance):\n            #print(number_iterations)\n            break\n# Return PageRanks of all the nodes\n    return output_ranks","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to return the PageRank of all nodes in a given webgraph G using the matrix power method\ndef PageRankPowerMethod(G, damping_factor = 0.85, error_tolerance = 0.1, max_iterations = 100):\n# Initialise number of iterations of algorithm to 0 and initialise a square matrix of size (number_of_nodes * number_of_nodes)\n    number_iterations = 0\n    number_of_nodes = G.number_of_nodes()\n    GoogleMatrix = np.zeros((number_of_nodes,number_of_nodes))\n# Initialise PageRank of all nodes to be initially the same value i.e 1/number_of_nodes\n    InputRanks = np.full((number_of_nodes,1),1/number_of_nodes)\n# Procedure to fill in values of matrix GoogleMatrix:\n    # GoogleMatrix[i][j] = 0 if no link exists from node j to node i\n    # GoogleMatrix[i][j] = 1/number of links from node j if link exists from node j to node i\n    # In case of a dangling node j, randomly move to any of the nodes i.e GoogleMatrix[i][j] = 1/number_of_nodes for all i\n    for i in range(0,number_of_nodes):\n        number_out_edges = G.out_degree(i+1)\n        if number_out_edges == 0:\n            for j in range(0,number_of_nodes):\n                GoogleMatrix[j][i] = 1/number_of_nodes\n        else:\n            successors = list(G.successors(i+1))\n            for j in range(0,number_out_edges):\n                GoogleMatrix[successors[j]-1][i] = 1/number_out_edges\n# Iterations of PageRank Algorithm\n    while number_iterations < max_iterations:\n        number_iterations = number_iterations + 1\n# Pagerank in next iteration = d(G * R) + (1-d)/N\n        OutputRanks = np.matmul(GoogleMatrix,InputRanks) * damping_factor + ((1-damping_factor)/(number_of_nodes))\n# Calculate difference between PageRanks of current and next iteration to check for convergence\n        difference = sum(sum(abs(OutputRanks - InputRanks)))\n        #print(difference)\n# Update PageRanks\n        InputRanks = OutputRanks\n# Check for convergence against a tolerance value\n        if(difference < error_tolerance):\n            #print(number_iterations)\n            break\n# Return PageRanks of all the nodes\n    return OutputRanks \n    ","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def PageRankInnerOuterMethod(G, damping_factor_alpha = 0.85, damping_factor_beta = 0.5, error_tolerance_outer = 0.1,error_tolerance_inner = 0.1, max_iterations_outer = 100, max_iterations_inner = 100):\n# Inner Outer iteration solves for a smaller damping factor first and then uses that result to solve for the given damping factor\n    if (damping_factor_alpha < damping_factor_beta):\n        return \"Error: Alpha must be greater than Beta\"\n# Initialise number of iterations of algorithm to 0 and initialise a square matrix of size (number_of_nodes * number_of_nodes)\n    number_iterations = 0\n    number_of_nodes = G.number_of_nodes()\n    GoogleMatrix = np.zeros((number_of_nodes,number_of_nodes))\n# Initialise PageRank of all nodes to be initially the same value i.e 1/number_of_nodes\n    InputRanks = np.full((number_of_nodes,1),1/number_of_nodes)\n# Procedure to fill in values of matrix GoogleMatrix:\n# GoogleMatrix[i][j] = 0 if no link exists from node j to node i\n# GoogleMatrix[i][j] = 1/number of links from node j if link exists from node j to node i\n# In case of a dangling node j, randomly move to any of the nodes i.e GoogleMatrix[i][j] = 1/number_of_nodes for all i\n    for i in range(0,number_of_nodes):\n        number_out_edges = G.out_degree(i+1)\n        if number_out_edges == 0:\n            for j in range(0,number_of_nodes):\n                GoogleMatrix[j][i] = 1/number_of_nodes\n        else:\n            successors = list(G.successors(i+1))\n            for j in range(0,number_out_edges):\n                GoogleMatrix[successors[j]-1][i] = 1/number_out_edges\n# Personalisation Vector set to be equal to 1/number_of_nodes for all nodes i.e all nodes are equally likely to be visited in the case of a random choice\n    personalisation_vector = np.full((number_of_nodes,1),1/number_of_nodes)\n# Initialise PageRanks to be equal to the Personalisation Vector i.e all equal\n    InputRanks = personalisation_vector\n# y vector used during Inner-Outer Iteration\n    y = np.matmul(GoogleMatrix,InputRanks)\n# Pagerank in next Outer iteration = alpha(G * R) + (1-alpha)/N\n    OutputRanks = (damping_factor_alpha * y + (1-damping_factor_alpha) * personalisation_vector)\n# Calculate difference between PageRanks of current and next Outer iteration to check for convergence\n    difference_outer = sum(sum(abs(OutputRanks - InputRanks)))\n# Repeat Outer iterations until convergence\n    while (difference_outer > error_tolerance_outer):\n# Calculate f as (alpha-beta)y + (1-alpha)/N\n# f used in Inner iterations\n        f = (damping_factor_alpha - damping_factor_beta) * y + (1 - damping_factor_alpha) * personalisation_vector\n# Set difference_inner to 100 to ensure atleast 1 iteration of inner loop\n        difference_inner = 100\n# Repeat Inner iterations until convergence \n        while (difference_inner > error_tolerance_inner):\n# Update InputRanks and y in each Inner iteration till the InputRanks converge for smaller damping factor beta \n            InputRanks = f + (damping_factor_beta * y)\n            y = np.matmul(GoogleMatrix,InputRanks)\n# Calculate difference between PageRanks of current and next Inner iteration to check for convergence\n            difference_inner = sum(sum(abs(f + (damping_factor_beta * y) - InputRanks)))\n# Update OutputRanks in each Outer iteration till the OutputRanks and InputRanks converge for damping factor alpha\n        OutputRanks = (damping_factor_alpha * y + (1-damping_factor_alpha) * personalisation_vector)\n        difference_outer = sum(sum(abs(OutputRanks - InputRanks)))\n# Set final PageRanks of all nodes in OutputRanks\n    OutputRanks = (damping_factor_alpha * y + (1-damping_factor_alpha) * personalisation_vector)\n# Return PageRanks of all the nodes\n    return OutputRanks","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to return the PageRank of all nodes in a given webgraph G using the improved matrix power method\ndef PageRankPowerMethodSparse(G, damping_factor = 0.85, error_tolerance = 0.1, max_iterations = 100):\n# Initialise number of iterations of algorithm to 0 and initialise 3 lists for rows,columns and data of the sparse matrix\n    number_iterations = 0\n    number_of_nodes = G.number_of_nodes()\n    rows = []\n    columns = []\n    data = []\n# Initialise PageRank of all nodes to be initially the same value i.e 1/number_of_nodes\n    InputRanks = np.full((number_of_nodes,1),1/number_of_nodes)\n# Procedure to fill in values of matrix GoogleMatrix:\n    # GoogleMatrix[i][j] = 0 if no link exists from node j to node i\n    # GoogleMatrix[i][j] = 1/number of links from node j if link exists from node j to node i\n    # In case of a dangling node j, randomly move to any of the nodes i.e GoogleMatrix[i][j] = 1/number_of_nodes for all i\n    for i in range(0,number_of_nodes):\n        number_out_edges = G.out_degree(i+1)\n        if number_out_edges == 0:\n            for j in range(0,number_of_nodes):\n                rows.append(j)\n                columns.append(i)\n                data.append(1/number_of_nodes)\n        else:\n            successors = list(G.successors(i+1))\n            for j in range(0,number_out_edges):\n                rows.append(successors[j]-1)\n                columns.append(i)\n                data.append(1/number_out_edges)\n# Construct GoogleMatrixSparse in Compressed Sparse Matrix(CSR) format using the data,rows and columns lists.\n    GoogleMatrixSparse = csr_matrix((data,(rows,columns)), shape=(number_of_nodes,number_of_nodes))\n# Iterations of PageRank Algorithm\n    while number_iterations < max_iterations:\n        number_iterations = number_iterations + 1\n# Pagerank in next iteration = d(G * R) + (1-d)/N\n        OutputRanks = GoogleMatrixSparse.dot(InputRanks) * damping_factor + ((1-damping_factor)/(number_of_nodes))\n# Calculate difference between PageRanks of current and next iteration to check for convergence\n        difference = sum(sum(abs(OutputRanks - InputRanks)))\n        #print(difference)\n# Update PageRanks\n        InputRanks = OutputRanks\n# Check for convergence against a tolerance value\n        if(difference < error_tolerance):\n            #print(number_iterations)\n            break\n# Return PageRanks of all the nodes\n    return OutputRanks \n    ","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to analyze PageRank accuracy and computation time for different damping factor values\ndef DampingFactorAnalysis():\n# Initially set damping factor to 1 and other parameters for the PageRank Algorithm   \n    curr_damping_factor = 1\n    error_tolerance = 0.001\n    max_iterations = 100\n    difference = 0\n    true_pageranks = 0\n    max_time = 0\n# Run PageRank Algorithm for different values of damping factor and find which value leads to fastest convergence\n    for i in range(0,11):\n        curr_damping_factor = 1 - (i/10)\n# Load a random graph with 20000 nodes and 1/1000th probability of an edge being added between 2 nodes\n        G = nx.fast_gnp_random_graph(20000,1/1000).to_directed()\n        G = nx.convert_node_labels_to_integers(G,1)\n        number_of_nodes = G.number_of_nodes()\n# Measure time taken for the PageRank to be computed using the iterative method\n        start_time = t.process_time()\n        pageranks = PageRankPowerMethod(G, curr_damping_factor, error_tolerance, max_iterations)\n        end_time = t.process_time()\n        total_time = end_time - start_time\n# Set maximum time to time if i is 0\n        if (i == 0):\n            max_time = total_time\n            true_pageranks = pageranks\n        error = 0.0\n        for j in range(0,number_of_nodes):\n            error = difference + abs(true_pageranks[j] - pageranks[j])\n        print(\"Damping Factor: \",(10-i)/10, \"Time Taken: \", total_time, \"Error compared to True PageRank: \", error)","execution_count":33,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Load a DiGraph where nodes are labelled from 1 to number_of_nodes\ndef LoadGraph():\n# Create an empty directed graph\n    G = nx.DiGraph()\n# Use the Stanford web graph from 2002 as a large dataset\n    dataset = open(\"../input/webgraphs/web-Stanford.txt\",\"r\")\n    i = 0\n    for line in dataset:\n        i = i+1\n# Information about graph in first 4 lines\n        if i < 5:\n            continue\n        else:\n# Add each edge to the graph\n            node1, node2 = (int(val) for val in line.split())\n            G.add_edge(node1,node2)\n    G = nx.convert_node_labels_to_integers(G,1)\n    return G","execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check that graph has successfully created all nodes and edges\nG = LoadGraph()\nprint(G.number_of_nodes())\nprint(G.number_of_edges())","execution_count":8,"outputs":[{"output_type":"stream","text":"281903\n2312497\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Analyze different damping factors\nDampingFactorAnalysis()","execution_count":34,"outputs":[{"output_type":"stream","text":"Damping Factor:  1.0 Time Taken:  4.700951138000164 Error compared to True PageRank:  [0.]\nDamping Factor:  0.9 Time Taken:  3.035951218000264 Error compared to True PageRank:  [1.31736264e-05]\nDamping Factor:  0.8 Time Taken:  2.8101013580003382 Error compared to True PageRank:  [3.00174591e-06]\nDamping Factor:  0.7 Time Taken:  2.8382574340002975 Error compared to True PageRank:  [8.05481481e-06]\nDamping Factor:  0.6 Time Taken:  2.6750187079996977 Error compared to True PageRank:  [2.06254881e-08]\nDamping Factor:  0.5 Time Taken:  2.6565456550001727 Error compared to True PageRank:  [7.94722487e-06]\nDamping Factor:  0.4 Time Taken:  2.464489378000053 Error compared to True PageRank:  [5.20967198e-06]\nDamping Factor:  0.3 Time Taken:  2.5023031830000946 Error compared to True PageRank:  [2.98449149e-06]\nDamping Factor:  0.2 Time Taken:  2.5237939290000213 Error compared to True PageRank:  [4.99381086e-06]\nDamping Factor:  0.1 Time Taken:  2.340845690000151 Error compared to True PageRank:  [3.47027975e-06]\nDamping Factor:  0.0 Time Taken:  2.1020001020001473 Error compared to True PageRank:  [2.19005726e-06]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"damping_factor = 0.85\nerror_tolerance = 0.1\nmax_iterations = 100\n#pageranks_power = PageRankPowerMethod(G, damping_factor, error_tolerance, max_iterations)\npageranks_iterative = PageRankIterative(G, damping_factor, error_tolerance, max_iterations)\nprint(pageranks_iterative[0],pageranks_iterative[1],pageranks_iterative[2], sum(pageranks_iterative))\n#print(pageranks_power[0],pageranks_power[1],pageranks_power[2],sum(pageranks_power))","execution_count":23,"outputs":[{"output_type":"stream","text":"5.333815740521636e-07 2.8607493192967646e-06 2.8607493192967646e-06 0.9999999999989433\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"H = nx.DiGraph()\nH.add_edge(1,2)\nH.add_edge(2,3)\nH.add_edge(3,1)\nH.add_edge(1,4)\nH.add_edge(2,4)\nH.add_edge(3,4)\ndamping_factor = 1\nerror_tolerance = 0.1\nmax_iterations = 100\npageranks_power = PageRankPowerMethod(H, damping_factor, error_tolerance, max_iterations)\npageranks_iterative = PageRankIterative(H, damping_factor, error_tolerance, max_iterations)\npageranks_inner_outer = PageRankInnerOuterMethod(H,damping_factor)\nprint(pageranks_power)\nprint(pageranks_iterative)\nprint(pageranks_inner_outer)\n","execution_count":24,"outputs":[{"output_type":"stream","text":"[[0.203125]\n [0.203125]\n [0.203125]\n [0.390625]]\n[0.203125, 0.203125, 0.203125, 0.390625]\n[[0.203125]\n [0.203125]\n [0.203125]\n [0.390625]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"I = nx.les_miserables_graph().to_directed()\nI = nx.convert_node_labels_to_integers(I,1)\ndamping_factor = 1\nerror_tolerance = 0.1\nmax_iterations = 100\npageranks_power = PageRankPowerMethod(I, damping_factor, error_tolerance, max_iterations)\npageranks_iterative = PageRankIterative(I, damping_factor, error_tolerance, max_iterations)\npageranks_inner_outer = PageRankInnerOuterMethod(I,damping_factor)\nprint(sum(pageranks_power))\nprint(sum(pageranks_iterative))\nprint(sum(pageranks_inner_outer))","execution_count":25,"outputs":[{"output_type":"stream","text":"[1.]\n1.0000000000000002\n[1.]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"I = nx.fast_gnp_random_graph(20000,1/1000).to_directed()\nI = nx.convert_node_labels_to_integers(I,1)\nprint(I.number_of_nodes())\nprint(I.number_of_edges())\ndamping_factor = 0.85\nerror_tolerance = 0.1\nmax_iterations = 100\nstart_time = t.process_time()\npageranks = PageRankPowerMethod(I,damping_factor, error_tolerance, max_iterations)\nend_time = t.process_time()\ntime_before_sparse_matrix_vector_multiplication = end_time - start_time\nprint(\"PageRanks: \",pageranks)\nprint(\"Sum of PageRanks: \",sum(pageranks))\nprint(\"Time Taken: \",time_before_sparse_matrix_vector_multiplication)","execution_count":30,"outputs":[{"output_type":"stream","text":"20000\n400052\nPageRanks:  [[5.56615540e-05]\n [4.53666352e-05]\n [3.80024050e-05]\n ...\n [4.21529162e-05]\n [4.20769084e-05]\n [3.51532350e-05]]\nSum of PageRanks:  [1.]\nTime Taken:  2.290788250999867\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(I.number_of_nodes())\nprint(I.number_of_edges())\ndamping_factor = 0.85\nerror_tolerance = 0.1\nmax_iterations = 100\nstart_time = t.process_time()\npageranks = PageRankPowerMethodSparse(I,damping_factor,error_tolerance)\nend_time = t.process_time()\ntime_after_sparse_matrix_vector_multiplication = end_time - start_time\nprint(\"PageRanks: \",pageranks)\nprint(\"Sum of PageRanks: \",sum(pageranks))\nprint(\"Time Taken: \",time_after_sparse_matrix_vector_multiplication)","execution_count":31,"outputs":[{"output_type":"stream","text":"20000\n400052\nPageRanks:  [[5.56615540e-05]\n [4.53666352e-05]\n [3.80024050e-05]\n ...\n [4.21529162e-05]\n [4.20769084e-05]\n [3.51532350e-05]]\nSum of PageRanks:  [1.]\nTime Taken:  0.7757984439999746\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}